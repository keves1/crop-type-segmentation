{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd crop-type-segmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from shapely.geometry import Polygon\n",
    "from rastervision.core.data import (\n",
    "    RasterioSource,\n",
    "    MinMaxTransformer,\n",
    "    TemporalMultiRasterSource,\n",
    "    Scene,\n",
    "    SemanticSegmentationLabelSource,\n",
    "    ClassConfig,\n",
    "    NanTransformer,\n",
    "    ReclassTransformer,\n",
    ")\n",
    "from rastervision.pytorch_learner import SemanticSegmentationSlidingWindowGeoDataset\n",
    "from terratorch.models import PrithviModelFactory\n",
    "from terratorch.datasets import HLSBands\n",
    "from torchmetrics import JaccardIndex\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import wandb\n",
    "\n",
    "from cropland_data_layer_class_table import class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrithviSemanticSegmentation(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        in_channels,\n",
    "        num_frames,\n",
    "        decoder_num_convs,\n",
    "        img_size,\n",
    "        learning_rate,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        model_factory = PrithviModelFactory()\n",
    "        self.model = model_factory.build_model(\n",
    "            task=\"segmentation\",\n",
    "            backbone=\"prithvi_vit_100\",\n",
    "            decoder=\"FCNDecoder\",\n",
    "            decoder_num_convs=decoder_num_convs,\n",
    "            in_channels=in_channels,\n",
    "            bands=[\n",
    "                HLSBands.BLUE,\n",
    "                HLSBands.GREEN,\n",
    "                HLSBands.RED,\n",
    "                HLSBands.NIR_NARROW,\n",
    "                HLSBands.SWIR_1,\n",
    "                HLSBands.SWIR_2,\n",
    "            ],\n",
    "            num_classes=num_classes,\n",
    "            pretrained=True,\n",
    "            num_frames=num_frames,\n",
    "            head_dropout=0.0,\n",
    "            img_size=img_size,\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for param in self.model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.jaccard_index = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        model_output = self.model(x)\n",
    "        mask = model_output.output\n",
    "        loss = F.cross_entropy(mask, y)\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        pred = torch.argmax(mask, dim=1)\n",
    "        iou = self.jaccard_index(pred, y)\n",
    "        self.log(\"train/iou\", iou, on_step=False, on_epoch=True)\n",
    "        y_flat = y.flatten().cpu()\n",
    "        pred_flat = pred.flatten().cpu()\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "            y_flat, pred_flat, average=\"macro\"\n",
    "        )\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train/precision_macro\": precision_macro,\n",
    "                \"train/recall_macro\": recall_macro,\n",
    "                \"train/f1_macro\": f1_macro,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = (\n",
    "            precision_recall_fscore_support(y_flat, pred_flat, average=\"weighted\")\n",
    "        )\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train/precision_weighted\": precision_weighted,\n",
    "                \"train/recall_weighted\": recall_weighted,\n",
    "                \"train/f1_weighted\": f1_weighted,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        model_output = self.model(x)\n",
    "        mask = model_output.output\n",
    "        loss = F.cross_entropy(mask, y)\n",
    "        self.log(\"val/loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        pred = torch.argmax(mask, dim=1)\n",
    "        iou = self.jaccard_index(pred, y)\n",
    "        self.log(\"val/iou\", iou, on_step=False, on_epoch=True)\n",
    "        y_flat = y.flatten().cpu()\n",
    "        pred_flat = pred.flatten().cpu()\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "            y_flat, pred_flat, average=\"macro\"\n",
    "        )\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val/precision_macro\": precision_macro,\n",
    "                \"val/recall_macro\": recall_macro,\n",
    "                \"val/f1_macro\": f1_macro,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = (\n",
    "            precision_recall_fscore_support(y_flat, pred_flat, average=\"weighted\")\n",
    "        )\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val/precision_weighted\": precision_weighted,\n",
    "                \"val/recall_weighted\": recall_weighted,\n",
    "                \"val/f1_weighted\": f1_weighted,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the following configuration as needed\n",
    "config = {}\n",
    "config[\"num_classes\"] = 17\n",
    "config[\"num_frames\"] = 7\n",
    "config[\"months\"] = [2, 4, 5, 6, 7, 8, 9]\n",
    "config[\"img_size\"] = 224\n",
    "config[\"learning_rate\"] = 0.001\n",
    "config[\"decoder_num_convs\"] = 1\n",
    "config[\"in_channels\"] = 6\n",
    "config[\"channels\"] = [0, 1, 2, 3, 4, 5]\n",
    "config[\"batch_size\"] = 5\n",
    "config[\"wandb_project\"] = \"test\"\n",
    "config[\"wandb_name\"] = \"test\"\n",
    "config[\"max_epochs\"] = 1\n",
    "config[\"num_workers\"] = 4 if torch.cuda.is_available() else 0\n",
    "config[\"ckpt_path\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")\n",
    "months = config[\"months\"]\n",
    "months_regex = f\"Landsat9_Composite_2022_0[{''.join(map(str, months))}].tiff\"\n",
    "l9_images = sorted(data_dir.glob(\"Landsat9_Composite_2022_0*.tiff\"))\n",
    "l9_images = [img for img in l9_images if re.match(months_regex, img.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [item[\"Color\"] for item in class_info]\n",
    "names = [item[\"Description\"] for item in class_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class IDs to use classes that contain more than 1% of pixels. All other classes are \"Other\" (0).\n",
    "# All classes for developed areas are combined\n",
    "most_frequent_crops = {\n",
    "    3: 1,\n",
    "    6: 2,\n",
    "    24: 3,\n",
    "    36: 4,\n",
    "    37: 5,\n",
    "    54: 6,\n",
    "    61: 7,\n",
    "    75: 8,\n",
    "    76: 9,\n",
    "    111: 10,\n",
    "    142: 11,\n",
    "    152: 12,\n",
    "    176: 13,\n",
    "    195: 14,\n",
    "    220: 15,\n",
    "}\n",
    "developed_classes = [82, 121, 122, 123, 124]\n",
    "mapping = {}\n",
    "for item in class_info:\n",
    "    value = int(item[\"Value\"])\n",
    "    if value in most_frequent_crops:\n",
    "        mapping[value] = most_frequent_crops[value]\n",
    "    elif value in developed_classes:\n",
    "        mapping[value] = 16\n",
    "    else:\n",
    "        mapping[value] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_config = ClassConfig(names=names, colors=colors, null_class=\"Other\")\n",
    "label_source = SemanticSegmentationLabelSource(\n",
    "    raster_source=RasterioSource(\n",
    "        uris=\"data/Cropland_Data_Layer_2022.tiff\",\n",
    "        raster_transformers=[ReclassTransformer(mapping)],\n",
    "    ),\n",
    "    class_config=class_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_sources = []\n",
    "for image_uri in l9_images:\n",
    "    raster_sources.append(\n",
    "        RasterioSource(\n",
    "            str(image_uri),\n",
    "            channel_order=config[\"channels\"],\n",
    "            raster_transformers=[NanTransformer(to_value=0), MinMaxTransformer()],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = TemporalMultiRasterSource(raster_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = raster_sources[0].bbox.extent\n",
    "extent = extent.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.7\n",
    "train_aoi = Polygon.from_bounds(\n",
    "    ymin=0, ymax=int(extent[\"ymax\"] * train_percent), xmin=0, xmax=extent[\"xmax\"]\n",
    ")\n",
    "val_aoi = Polygon.from_bounds(\n",
    "    ymin=math.ceil(extent[\"ymax\"] * train_percent),\n",
    "    ymax=extent[\"ymax\"],\n",
    "    xmin=0,\n",
    "    xmax=extent[\"xmax\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scene = Scene(\n",
    "    id=\"train\",\n",
    "    raster_source=time_series,\n",
    "    label_source=label_source,\n",
    "    aoi_polygons=[train_aoi],\n",
    ")\n",
    "val_scene = Scene(\n",
    "    id=\"val\",\n",
    "    raster_source=time_series,\n",
    "    label_source=label_source,\n",
    "    aoi_polygons=[val_aoi],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SemanticSegmentationSlidingWindowGeoDataset(\n",
    "    train_scene, size=config[\"img_size\"], stride=config[\"img_size\"], padding=0\n",
    ")\n",
    "val_dataset = SemanticSegmentationSlidingWindowGeoDataset(\n",
    "    val_scene, size=config[\"img_size\"], stride=config[\"img_size\"], padding=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len train dataset: {len(train_dataset)}\")\n",
    "print(f\"len val dataset: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Changes the order of the axes from what Raster Vision outputs (B,T,C,H,W) to what\n",
    "    the Prithvi model expects (B,C,T,H,W).\n",
    "    \"\"\"\n",
    "    data, targets = zip(*batch)\n",
    "    data = torch.stack(data)\n",
    "    data = data.permute(0, 2, 1, 3, 4)\n",
    "    if isinstance(targets[0], torch.Tensor):\n",
    "        targets = torch.stack(targets)\n",
    "    else:\n",
    "        targets = torch.tensor(targets)\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    collate_fn=custom_collate_fn,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    collate_fn=custom_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = x[:, [2, 1, 0], 0, :, :]\n",
    "\n",
    "batch_size = images.shape[0]\n",
    "\n",
    "fig, axes = plt.subplots(2, batch_size, figsize=(3 * batch_size, 6))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    img = torch.squeeze(images[i])\n",
    "    img = images[i].permute(1, 2, 0).numpy()\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[0, i].set_title(f\"Image {i + 1}\")\n",
    "\n",
    "    mask = y[i].numpy()\n",
    "    axes[1, i].imshow(mask, cmap=\"tab20\", vmin=0, vmax=15)\n",
    "    axes[1, i].axis(\"off\")\n",
    "    axes[1, i].set_title(f\"Mask {i + 1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PrithviSemanticSegmentation(\n",
    "    num_classes=config[\"num_classes\"],\n",
    "    in_channels=config[\"in_channels\"],\n",
    "    num_frames=config[\"num_frames\"],\n",
    "    decoder_num_convs=config[\"decoder_num_convs\"],\n",
    "    img_size=config[\"img_size\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    name=config[\"wandb_name\"], save_dir=\"wandb-logs\", project=config[\"wandb_project\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=config[\"max_epochs\"],\n",
    "    log_every_n_steps=1,\n",
    "    # limit_train_batches=1,\n",
    "    # limit_test_batches=1,\n",
    "    # fast_dev_run=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    ckpt_path=config[\"ckpt_path\"],\n",
    ")\n",
    "wandb_logger.experiment.config.update(config)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
