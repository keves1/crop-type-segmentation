{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from shapely.geometry import Polygon\n",
    "from rastervision.core.data import RasterioSource, MinMaxTransformer, TemporalMultiRasterSource, Scene, SemanticSegmentationLabelSource, ClassConfig, NanTransformer, ReclassTransformer\n",
    "from rastervision.pytorch_learner import SemanticSegmentationSlidingWindowGeoDataset\n",
    "from terratorch.models import PrithviModelFactory\n",
    "from terratorch.datasets import HLSBands\n",
    "\n",
    "from cropland_data_layer_class_table import class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrithviSemanticSegmentation(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_factory = PrithviModelFactory()\n",
    "        self.model = model_factory.build_model(task=\"segmentation\",\n",
    "                backbone=\"prithvi_vit_100\",\n",
    "                decoder=\"FCNDecoder\",\n",
    "                decoder_num_convs = 1,\n",
    "                in_channels=6,\n",
    "                bands=[\n",
    "                    HLSBands.BLUE,\n",
    "                    HLSBands.GREEN,\n",
    "                    HLSBands.RED,\n",
    "                    HLSBands.NIR_NARROW,\n",
    "                    HLSBands.SWIR_1,\n",
    "                    HLSBands.SWIR_2,\n",
    "                ],\n",
    "                num_classes=16,\n",
    "                pretrained=True,\n",
    "                num_frames=7,\n",
    "                head_dropout=0.0,\n",
    "                img_size=224,\n",
    "            )\n",
    "\n",
    "        for param in self.model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        model_output = self.model(x)\n",
    "        mask = model_output.output\n",
    "        loss = F.cross_entropy(mask, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        model_output = self.model(x)\n",
    "        mask = model_output.output\n",
    "        loss = F.cross_entropy(mask, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/workspace/data')\n",
    "l9_images = sorted(data_dir.glob('Landsat9_Composite_2022_0[2-9].tiff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [item[\"Color\"] for item in class_info]\n",
    "names = [item[\"Description\"] for item in class_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class IDs to use classes that contain more than 1% of pixels. All other classes are \"Other\" (0).\n",
    "# All classes for developed areas are combined\n",
    "most_frequent_crops = {1:1, 6:2, 24:3, 28:4, 36:5, 37:6, 54:7, 61:8, 75:9, 76:10, 152:11, 176:12, 211:13, 220:14}\n",
    "developed_classes = [82, 121, 122, 123, 124]\n",
    "mapping = {}\n",
    "for item in class_info:\n",
    "    value = int(item[\"Value\"])\n",
    "    if value in most_frequent_crops:\n",
    "        mapping[value] = most_frequent_crops[value]\n",
    "    elif value in developed_classes:\n",
    "        mapping[value] = 15\n",
    "    else:\n",
    "        mapping[value] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_config = ClassConfig(names=names, colors=colors, null_class=\"Other\")\n",
    "label_source = SemanticSegmentationLabelSource(\n",
    "                raster_source=RasterioSource(uris='/workspace/data/Cropland_Data_Layer_2022.tiff',\n",
    "                                             raster_transformers=[ReclassTransformer(mapping)]\n",
    "                                             ), \n",
    "                class_config=class_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_sources = []\n",
    "for image_uri in l9_images:\n",
    "    raster_sources.append(RasterioSource(str(image_uri), \n",
    "                                         channel_order=[0, 1, 2, 3, 4, 5], \n",
    "                                         raster_transformers=[NanTransformer(to_value=0),\n",
    "                                                              MinMaxTransformer()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = TemporalMultiRasterSource(raster_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = raster_sources[0].bbox.extent\n",
    "extent = extent.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.7\n",
    "train_aoi = Polygon.from_bounds(ymin=0, ymax=int(extent[\"ymax\"]*train_percent), xmin=0, xmax=extent[\"xmax\"])\n",
    "val_aoi = Polygon.from_bounds(ymin=math.ceil(extent[\"ymax\"]*train_percent), ymax=extent[\"ymax\"], xmin=0, xmax=extent[\"xmax\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scene = Scene(id=\"train\", raster_source=time_series, label_source=label_source, aoi_polygons=[train_aoi])\n",
    "val_scene = Scene(id=\"val\", raster_source=time_series, label_source=label_source, aoi_polygons=[val_aoi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SemanticSegmentationSlidingWindowGeoDataset(train_scene, size=224, stride=224, padding=0)\n",
    "val_dataset = SemanticSegmentationSlidingWindowGeoDataset(val_scene, size=224, stride=224, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len train dataset: {len(train_dataset)}\")\n",
    "print(f\"len val dataset: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Changes the order of the axes from what Raster Vision outputs (B,T,C,H,W) to what\n",
    "    the Prithvi model expects (B,C,T,H,W).\n",
    "    \"\"\"\n",
    "    data, targets = zip(*batch)\n",
    "    data = torch.stack(data)\n",
    "    data = data.permute(0, 2, 1, 3, 4)\n",
    "    if isinstance(targets[0], torch.Tensor):\n",
    "        targets = torch.stack(targets)\n",
    "    else:\n",
    "        targets = torch.tensor(targets)\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_dl = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = x[:, [2,1,0], 0, :, :]\n",
    "\n",
    "batch_size = images.shape[0]\n",
    "\n",
    "fig, axes = plt.subplots(2, batch_size, figsize=(3 * batch_size, 6))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    img = torch.squeeze(images[i])\n",
    "    img = images[i].permute(1, 2, 0).numpy()\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title(f'Image {i + 1}')\n",
    "\n",
    "    mask = y[i].numpy()\n",
    "    axes[1, i].imshow(mask, cmap='tab20', vmin=0, vmax=15)\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title(f'Mask {i + 1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PrithviSemanticSegmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(save_dir='workspace/csv-logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=csv_logger,\n",
    "    max_epochs=1,\n",
    "    # fast_dev_run=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
